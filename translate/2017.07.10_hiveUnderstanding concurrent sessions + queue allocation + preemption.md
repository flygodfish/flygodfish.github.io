## [Hive - Understanding concurrent sessions + queue allocation + preemption](https://community.hortonworks.com/articles/56636/hive-understanding-concurrent-sessions-queue-alloc.html)


### 在这篇文章里：
- 展示了hive sessions是如何工作的
- 澄清了一些误解关于hive sessions的行为（大部分人，包括我知道最近，相信hive sessions和queue allocation不同于他们的实际工作方式）
- 一个有建设的解决办法给中小环境（多达200个节点），允许并发用户 + 多租户（单一软件程序为多个客户服务的架构）+ 软件分配 +　抢占权

### 假定：
在HDP2.4.0上测试
我正在使用的是最佳实践推荐的配置和安全性和Hive性能指南有关的文档（http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.4.0/bk_performance_tuning/content/ch_hive_architectural_overview.html）
包含以下配置：
- hive engine (hive.execution.engine) = tez
- hive do-as (hive.server2.enable.doAs) = false
- hive default queues (hive.server2.tez.default.queues) = (queue-name1,queue-name2,etc)
- hive number of sessions (hive.server2.tez.sessions.per.default.queue) = 1 (or up to 4)
- hive start sessions (hive.server2.tez.initialize.default.sessions) = true
- hive pre-warm containers (hive.prewarm.enabled) = true
- hive num-containers (hive.prewarm.numcontainers) = (some number >1)
- tez container reuse (tez.am.container.reuse.enabled) = true
所有的例子都是关于hiveserver2 + jdbc client (beeline/other jdbc client or odbc driver)，它并不适用于hive client或者其它能直接影响hive meta store的工具

### 理解#1 - 默认的队列 + 许多会话
当你定义hive默认队列（hive.server2.tez.default.queues）和hive的会话数x（hive.server2.tez.sessions.per.default.queue=x）和hive开始会话为真（hive.server2.tez.initialize.default.sessions=true）时，hiveserver2启动时将会创建一个Tez AM(application master)给每一个默认队列x个会话数。

举例，如果你定义默认队列是“queue1,queue2”和会话数为2，hiveserver2将会创建4个Tez AM(2个给queue1,2个给queue2)

如果你连续使用hiveserver2，这些Tez AM将会持续运行，但是如果你的hiveserver2是闲置的，这些Tez AM将会基于被定义的超时时间（tez.session.am.dag.submit.timeout.secs）被杀死。

### 理解#2 - 预热容器
预热容器不同于会话的初始化。

容器的数量与在默认情况下连接到每个Tez AM的YARN执行容器（并不考虑AM容器）数量有关。甚至当Tez AM空闲时，每个AM也将会持有相同数量的容器

再次检查视频#1，这是将注意力放在每个AM启动时容器的数量，将会注意到容器的数量总是等于你定义的容器数量+1（这个额外的容器运行的是AM - Application Master）

### 理解#3 - 当Tez AM初始化时（调用AM池）
如果你没有指定队列的名字（tez.queue.name），一个查询将只会使用池中的一个Tez AM（如上所述的初始），在这个情况中，Hiveserver2将只会选择一个空闲的或者可用的Tez AM（随机选择一个队列名）。如果使用相同的JDBC/ODBC执行多种查询时，每个查询都将在不同的Tez AM中执行和每个查询会在不同的队列名中被执行。

如果在你的JDBC/ODBC连接中制定了队列名，hiveserver2将会为该连接创建一个新的Tez AM并且不会使用任何初始化的会话。

PS：只有jdbc/odbc客户端可以使用初始的Tez AM，hive client（命令行）或者其它外部hive组件不能使用初始化的Tez AM。

### 理解#4 - 如果你有超过Tez AM初始化数量的并发查询，咋办？
如果你没有指定队列名（如上所述）hiveserver2将会持有你的查询直到默认的Tez AM(池)其中的一个可用于服务你的查询。这里将不会有任何的消息在JDBC/OBDC客户端和hiveserver2的日志文件中。一个不懂的用户（或者admin）可能认为JDBC/ODBC连接或者hiveserver2损坏了，但是它仅仅是等待Tez AM去执行查询。

如果你指定了队列名，那么不管有多少的Tez AM在被使用或者空闲都不重要，hiveserver2将为了这个连接创建一个新的Tez AM并且这个查询可被执行（如果队列有可用的资源）

### 理解#5 - 如何使用AM池定制队列名（用户定义）
在使用Tez AM池（初始会话）的同一时间没有办法允许用户指定队列名。如果你使用的用例要求不同的/专用的Tez AM池给不同组的用户，你需要专用的hiveserver2服务，每个服务都需要各自的默认队列名和会话数并要求每组的用户使用各自的hiveserver2.

### 理解#6 - 单个查询使用容器的数量
每个查询的容器数量将在这里被定义使用（https://cwiki.apache.org/confluence/display/TEZ/How+initial+task+parallelism+works ），考虑到当前队列的可用资源的数量，队列中可用资源的数量是由最小保证容量定义的。

换句话说，如果你执行一个承重的查询，这个查询可能会在这个队列中使用所有的可用资源，而没有给其他查询留下执行的空间。

### 理解#7 - 容量调度排序策略和抢占 
在队列中没有抢占并且公平的排序策略只在容器被AM释放才有效。如果你按照推荐的那样重用容器（tez.am.container.reuse.enabled=true），容器将仅在查询完成后通过AM被释放，如果第一个查询被执行的很承重，所有的后续查询将等待第一个查询的完成才能接受到“公平”的分享。

### 理解#8 - 所有的工作一起执行的场景
我的理解是这些部件和配置在最初的时候被设计好并且有了很好的优化，可以在以下的场景中工作：
1 - 有大量专用的资源给查询的大型集群
2 - 高并发查询/用户
3 - 多hiveserver2实例化，举例，一个轻量查询，其他都是承重查询
4 - 固定数量的会话（Tez AM）和每个会话固定数量的容器,所有的都是预热的（这些数量会因每种查询/hiveserver2而不同）
5 - 并不需要公平排序策略或者抢占因为所有的查询总是在预热的容器中使用

#### 中小集群最常见的需求
虽然在第8节中描述的场景非常有趣（在那种规模最好的解决办法是hive+tez），但在大部分的中小Hadoop集群那是不现实的,通常采用阶级和POCs。

以我的经验，大部分像这种环境的用户的需求更简单些，像这样：
1 - 多租用集群，让我们说成生产和用户层共享资源
2 - 软限制和抢占（在晚上，用户离线，给生产100%的资源，直到白天，保持生产的最小可运行分配以完成SLA，但给于用户最大的资源）
3 - 公平分配（如果只有一个用户跑一个资源，那就给他全部的资源，当有第二，第三和后续用户执行查询，给他们同等数量的资源分配，但是尊重不同队列最小的份额，这代表了优先级）

#### 建议的解决办法/设置
Hive+Tez是上述2个场景中完美的解决方案，但是现在，我将要分享调整参数关于中小集群常见的需求的细节。
1 - YARN 如这里描述的开启抢占（http://hortonworks.com/blog/better-slas-via-resource-preemption-in-yarns-capacityscheduler/ ）

我yarn的配置：
yarn.resourcemanager.scheduler.monitor.enable=trueyarn.resourcemanager.monitor.capacity.preemption.max_ignored_over_capacity=0.01
yarn.resourcemanager.monitor.capacity.preemption.max_wait_before_kill=1000
yarn.resourcemanager.monitor.capacity.preemption.monitoring_interval=1000
yarn.resourcemanager.monitor.capacity.preemption.natural_termination_factor=1
yarn.resourcemanager.monitor.capacity.preemption.total_preemption_per_round=1
yarn.resourcemanager.scheduler.monitor.policies=org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy

2 - 容量调度 给每个并发会话创建一个查询，举例，如果你想要三个并发用户，创建3个查询（这需要实现完全抢占）
yarn.scheduler.capacity.maximum-am-resource-percent=0.2
yarn.scheduler.capacity.maximum-applications=10000
yarn.scheduler.capacity.node-locality-delay=40
yarn.scheduler.capacity.queue-mappings-override.enable=false
yarn.scheduler.capacity.root.accessible-node-labels=*
yarn.scheduler.capacity.root.acl_administer_queue=*
yarn.scheduler.capacity.root.capacity=100
yarn.scheduler.capacity.root.default.acl_submit_applications=*
yarn.scheduler.capacity.root.default.capacity=30
yarn.scheduler.capacity.root.default.maximum-capacity=100
yarn.scheduler.capacity.root.default.state=RUNNING
yarn.scheduler.capacity.root.default.user-limit-factor=10
yarn.scheduler.capacity.root.hive-test.acl_administer_queue=*
yarn.scheduler.capacity.root.hive-test.acl_submit_applications=*
yarn.scheduler.capacity.root.hive-test.capacity=70
yarn.scheduler.capacity.root.hive-test.maximum-capacity=70
yarn.scheduler.capacity.root.hive-test.minimum-user-limit-percent=100
yarn.scheduler.capacity.root.hive-test.ordering-policy=fifo
yarn.scheduler.capacity.root.hive-test.production.acl_administer_queue=*
yarn.scheduler.capacity.root.hive-test.production.acl_submit_applications=*
yarn.scheduler.capacity.root.hive-test.production.capacity=40
yarn.scheduler.capacity.root.hive-test.production.maximum-capacity=100
yarn.scheduler.capacity.root.hive-test.production.minimum-user-limit-percent=100
yarn.scheduler.capacity.root.hive-test.production.ordering-policy=fifo
yarn.scheduler.capacity.root.hive-test.production.state=RUNNING
yarn.scheduler.capacity.root.hive-test.production.user-limit-factor=100
yarn.scheduler.capacity.root.hive-test.queues=production,user
yarn.scheduler.capacity.root.hive-test.state=RUNNING
yarn.scheduler.capacity.root.hive-test.user-limit-factor=10
yarn.scheduler.capacity.root.hive-test.user.acl_administer_queue=*
yarn.scheduler.capacity.root.hive-test.user.acl_submit_applications=*
yarn.scheduler.capacity.root.hive-test.user.capacity=60
yarn.scheduler.capacity.root.hive-test.user.maximum-capacity=100
yarn.scheduler.capacity.root.hive-test.user.minimum-user-limit-percent=100
yarn.scheduler.capacity.root.hive-test.user.ordering-policy=fifo
yarn.scheduler.capacity.root.hive-test.user.queues=user1,user2,user3
yarn.scheduler.capacity.root.hive-test.user.state=RUNNING
yarn.scheduler.capacity.root.hive-test.user.user-limit-factor=100
yarn.scheduler.capacity.root.hive-test.user.user1.acl_administer_queue=*
yarn.scheduler.capacity.root.hive-test.user.user1.acl_submit_applications=*
yarn.scheduler.capacity.root.hive-test.user.user1.capacity=33
yarn.scheduler.capacity.root.hive-test.user.user1.maximum-capacity=100
yarn.scheduler.capacity.root.hive-test.user.user1.minimum-user-limit-percent=100
yarn.scheduler.capacity.root.hive-test.user.user1.ordering-policy=fifo
yarn.scheduler.capacity.root.hive-test.user.user1.state=RUNNING
yarn.scheduler.capacity.root.hive-test.user.user1.user-limit-factor=10
yarn.scheduler.capacity.root.hive-test.user.user2.acl_administer_queue=*
yarn.scheduler.capacity.root.hive-test.user.user2.acl_submit_applications=*
yarn.scheduler.capacity.root.hive-test.user.user2.capacity=33
yarn.scheduler.capacity.root.hive-test.user.user2.maximum-capacity=100
yarn.scheduler.capacity.root.hive-test.user.user2.minimum-user-limit-percent=100
yarn.scheduler.capacity.root.hive-test.user.user2.ordering-policy=fifo
yarn.scheduler.capacity.root.hive-test.user.user2.state=RUNNING
yarn.scheduler.capacity.root.hive-test.user.user2.user-limit-factor=10
yarn.scheduler.capacity.root.hive-test.user.user3.acl_administer_queue=*
yarn.scheduler.capacity.root.hive-test.user.user3.acl_submit_applications=*
yarn.scheduler.capacity.root.hive-test.user.user3.capacity=34
yarn.scheduler.capacity.root.hive-test.user.user3.maximum-capacity=100
yarn.scheduler.capacity.root.hive-test.user.user3.minimum-user-limit-percent=100
yarn.scheduler.capacity.root.hive-test.user.user3.ordering-policy=fifo
yarn.scheduler.capacity.root.hive-test.user.user3.state=RUNNING
yarn.scheduler.capacity.root.hive-test.user.user3.user-limit-factor=10
yarn.scheduler.capacity.root.queues=default,hive-test

3 - Hive/Tez 定义默认的队列和每个队列一个初始化的会话
hive.server2.tez.default.queues=user1,user2,user3
set hive.server2.tez.sessions.per.default.queue=1
set hive.server2.tez.initialize.default.sessions=true
set hive.prewarm.enabled=true
set hive.prewarm.numcontainers=2
set tez.am.container.reuse.enabled=true

最后，在Tez设置中增加释放闲置容器超时也很有用，这些将帮助容器在查询完后持续运行一小段时间（不仅是预热的容器，但是由AM加载的额外的容器），可以使后续查询初始化时间更快。建议：
tez.am.container.idle.release-timeout-min.millis=30000
tez.am.container.idle.release-timeout-max.millis=90000






